{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 different words in your corpus\n",
      "X_pad.shape (3, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.,  6.,  0.,  0.,  0.],\n",
       "       [ 1.,  2.,  7.,  4.,  8.,  9., 10., 11.],\n",
       "       [12.,  3.,  5., 13., 14., 15.,  5., 16.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "### Let's create some mock data\n",
    "def get_mock_up_data():\n",
    "    sentence_1 = 'Deep learning is super easy'\n",
    "    sentence_2 = 'Deep learning was super bad and too long'\n",
    "    sentence_3 = 'This is the best lecture of the camp!'\n",
    "\n",
    "    X = [sentence_1, sentence_2, sentence_3]\n",
    "    y = np.array([1., 0., 1.])\n",
    "\n",
    "    ### Let's tokenize the vocabulary \n",
    "    tk = Tokenizer()\n",
    "    tk.fit_on_texts(X)\n",
    "    vocab_size = len(tk.word_index)\n",
    "    print(f'There are {vocab_size} different words in your corpus')\n",
    "    X_token = tk.texts_to_sequences(X)\n",
    "\n",
    "    ### Pad the inputs\n",
    "    X_pad = pad_sequences(X_token, dtype='float32', padding='post')\n",
    "    \n",
    "    return X_pad, y, vocab_size\n",
    "\n",
    "X_pad, y, vocab_size = get_mock_up_data()\n",
    "print(\"X_pad.shape\", X_pad.shape)\n",
    "X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 8, 100)            1700      \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 20)                9680      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,401\n",
      "Trainable params: 11,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Let's build the neural network now\n",
    "from keras.api._v2.keras import layers, Sequential\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size + 1, # 16 + 1 for the 0 padding\n",
    "    input_length=8, # Max_sentence_length (optional, for model summary)\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer\n",
    "))\n",
    "\n",
    "model.add(layers.LSTM(20))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of parameters: 1700\n"
     ]
    }
   ],
   "source": [
    "print(f'Expected number of parameters: {(vocab_size + 1) * embedding_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5892 - accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5482 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5130 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4800 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4485 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_pad, y, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d405f1e39694aa8a34b8452efbe5345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570e00f2e5b449fea79c6a8d1f1374c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e2da816e0340ce84f2d844ad15d614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae67bc533f74997bcd6d19db0c6ffaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b48cf9a355540348e392815f04239f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteQ5FVXW/imdb_reviews-train.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ea971d286549929125edac93d95a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96dce2e906014b759612a090e7fc7dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteQ5FVXW/imdb_reviews-test.tfrecord*...:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2e2c1463eb463499f1a2a814d6227f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8957aec4e34c4ee3957e7810e9fbdad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteQ5FVXW/imdb_reviews-unsupervised.tfrec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
       "       b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's get some text first\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], \n",
    "                                            batch_size=-1, as_supervised=True)\n",
    "\n",
    "train_sentences, train_labels = tfds.as_numpy(train_data)\n",
    "test_sentences, test_labels = tfds.as_numpy(test_data)\n",
    "\n",
    "# Let's check two sentences\n",
    "train_sentences[0:2]\n",
    "\n",
    "# We have to convert the sentences into list of words! The computer won't do it for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this',\n",
       "  'was',\n",
       "  'an',\n",
       "  'absolutely',\n",
       "  'terrible',\n",
       "  'movie',\n",
       "  \"don't\",\n",
       "  'be',\n",
       "  'lured',\n",
       "  'in',\n",
       "  'by',\n",
       "  'christopher',\n",
       "  'walken',\n",
       "  'or',\n",
       "  'michael',\n",
       "  'ironside',\n",
       "  'both',\n",
       "  'are',\n",
       "  'great',\n",
       "  'actors',\n",
       "  'but',\n",
       "  'this',\n",
       "  'must',\n",
       "  'simply',\n",
       "  'be',\n",
       "  'their',\n",
       "  'worst',\n",
       "  'role',\n",
       "  'in',\n",
       "  'history',\n",
       "  'even',\n",
       "  'their',\n",
       "  'great',\n",
       "  'acting',\n",
       "  'could',\n",
       "  'not',\n",
       "  'redeem',\n",
       "  'this',\n",
       "  \"movie's\",\n",
       "  'ridiculous',\n",
       "  'storyline',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'an',\n",
       "  'early',\n",
       "  'nineties',\n",
       "  'us',\n",
       "  'propaganda',\n",
       "  'piece',\n",
       "  'the',\n",
       "  'most',\n",
       "  'pathetic',\n",
       "  'scenes',\n",
       "  'were',\n",
       "  'those',\n",
       "  'when',\n",
       "  'the',\n",
       "  'columbian',\n",
       "  'rebels',\n",
       "  'were',\n",
       "  'making',\n",
       "  'their',\n",
       "  'cases',\n",
       "  'for',\n",
       "  'revolutions',\n",
       "  'maria',\n",
       "  'conchita',\n",
       "  'alonso',\n",
       "  'appeared',\n",
       "  'phony',\n",
       "  'and',\n",
       "  'her',\n",
       "  'pseudo',\n",
       "  'love',\n",
       "  'affair',\n",
       "  'with',\n",
       "  'walken',\n",
       "  'was',\n",
       "  'nothing',\n",
       "  'but',\n",
       "  'a',\n",
       "  'pathetic',\n",
       "  'emotional',\n",
       "  'plug',\n",
       "  'in',\n",
       "  'a',\n",
       "  'movie',\n",
       "  'that',\n",
       "  'was',\n",
       "  'devoid',\n",
       "  'of',\n",
       "  'any',\n",
       "  'real',\n",
       "  'meaning',\n",
       "  'i',\n",
       "  'am',\n",
       "  'disappointed',\n",
       "  'that',\n",
       "  'there',\n",
       "  'are',\n",
       "  'movies',\n",
       "  'like',\n",
       "  'this',\n",
       "  'ruining',\n",
       "  \"actor's\",\n",
       "  'like',\n",
       "  'christopher',\n",
       "  \"walken's\",\n",
       "  'good',\n",
       "  'name',\n",
       "  'i',\n",
       "  'could',\n",
       "  'barely',\n",
       "  'sit',\n",
       "  'through',\n",
       "  'it'],\n",
       " ['i',\n",
       "  'have',\n",
       "  'been',\n",
       "  'known',\n",
       "  'to',\n",
       "  'fall',\n",
       "  'asleep',\n",
       "  'during',\n",
       "  'films',\n",
       "  'but',\n",
       "  'this',\n",
       "  'is',\n",
       "  'usually',\n",
       "  'due',\n",
       "  'to',\n",
       "  'a',\n",
       "  'combination',\n",
       "  'of',\n",
       "  'things',\n",
       "  'including',\n",
       "  'really',\n",
       "  'tired',\n",
       "  'being',\n",
       "  'warm',\n",
       "  'and',\n",
       "  'comfortable',\n",
       "  'on',\n",
       "  'the',\n",
       "  'sette',\n",
       "  'and',\n",
       "  'having',\n",
       "  'just',\n",
       "  'eaten',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'however',\n",
       "  'on',\n",
       "  'this',\n",
       "  'occasion',\n",
       "  'i',\n",
       "  'fell',\n",
       "  'asleep',\n",
       "  'because',\n",
       "  'the',\n",
       "  'film',\n",
       "  'was',\n",
       "  'rubbish',\n",
       "  'the',\n",
       "  'plot',\n",
       "  'development',\n",
       "  'was',\n",
       "  'constant',\n",
       "  'constantly',\n",
       "  'slow',\n",
       "  'and',\n",
       "  'boring',\n",
       "  'things',\n",
       "  'seemed',\n",
       "  'to',\n",
       "  'happen',\n",
       "  'but',\n",
       "  'with',\n",
       "  'no',\n",
       "  'explanation',\n",
       "  'of',\n",
       "  'what',\n",
       "  'was',\n",
       "  'causing',\n",
       "  'them',\n",
       "  'or',\n",
       "  'why',\n",
       "  'i',\n",
       "  'admit',\n",
       "  'i',\n",
       "  'may',\n",
       "  'have',\n",
       "  'missed',\n",
       "  'part',\n",
       "  'of',\n",
       "  'the',\n",
       "  'film',\n",
       "  'but',\n",
       "  'i',\n",
       "  'watched',\n",
       "  'the',\n",
       "  'majority',\n",
       "  'of',\n",
       "  'it',\n",
       "  'and',\n",
       "  'everything',\n",
       "  'just',\n",
       "  'seemed',\n",
       "  'to',\n",
       "  'happen',\n",
       "  'of',\n",
       "  'its',\n",
       "  'own',\n",
       "  'accord',\n",
       "  'without',\n",
       "  'any',\n",
       "  'real',\n",
       "  'concern',\n",
       "  'for',\n",
       "  'anything',\n",
       "  'else',\n",
       "  'i',\n",
       "  'cant',\n",
       "  'recommend',\n",
       "  'this',\n",
       "  'film',\n",
       "  'at',\n",
       "  'all']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's convert the list of sentences to a list of lists of words with a Keras utility function\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "\n",
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# This line trains an entire embedding for the words in your train set\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23731543,  0.20836844,  0.93226045, -0.39881945, -0.1734969 ,\n",
       "        0.35311407,  0.17482664, -0.60660696, -1.0988765 , -1.0357668 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the representation of any word\n",
    "\n",
    "word2vec.wv['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.9684410691261292),\n",
       " ('thing', 0.9233741164207458),\n",
       " ('sequel', 0.9121431708335876),\n",
       " ('experience', 0.8882043361663818),\n",
       " ('it', 0.8812832832336426),\n",
       " ('documentary', 0.8812153935432434),\n",
       " ('word', 0.8791374564170837),\n",
       " ('comment', 0.8772283792495728),\n",
       " ('still', 0.8713523149490356),\n",
       " ('ending', 0.8626609444618225)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's look at the 10 closest words to `movie`\n",
    "\n",
    "word2vec.wv.most_similar('movie', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the size of the embedding space, you just have to set-up the `vector_size` keyword\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train[:1000], vector_size=50) # We keep the training short by taking only 1000 sentences\n",
    "\n",
    "len(word2vec.wv['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word seen only less than 5 times, excluded from corpus\n"
     ]
    }
   ],
   "source": [
    "# The Word2Vec learns a representation for words that are present more than `min_count` number of times\n",
    "# This is to prevent learning representations based on a few examples only\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train[:1000], vector_size=50, min_count=5)\n",
    "\n",
    "try: \n",
    "    len(word2vec.wv['columbian'])\n",
    "except:\n",
    "    print(\"word seen only less than 5 times, excluded from corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=X_train[:10000], vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7151367 , -1.3908328 ,  0.7818068 ,  2.697015  ,  1.9684062 ,\n",
       "        1.6081227 ,  0.5611543 ,  2.1747055 , -1.0605388 , -1.263652  ,\n",
       "       -0.88034153,  0.04986176, -1.0676707 ,  2.012395  , -1.7021823 ,\n",
       "        0.42320666, -1.5596554 ,  2.6457067 ,  0.8514581 ,  0.29461867,\n",
       "        2.6989698 , -0.07449581,  0.65059406, -0.1270424 ,  0.01579017,\n",
       "       -0.554168  , -0.80333984, -0.7256611 ,  0.53043145, -2.2746706 ,\n",
       "        2.1711276 ,  1.196995  , -0.4668084 ,  1.5948262 ,  0.59737444,\n",
       "       -0.251413  , -0.4915388 ,  0.43606195, -2.6461565 , -1.8219426 ,\n",
       "        0.15553944, -0.93499   , -0.57123417,  0.05773273,  1.4741597 ,\n",
       "        0.43062705, -1.7170299 , -1.9491525 ,  2.9769535 ,  0.24243897,\n",
       "       -0.8985411 , -1.2618579 , -1.3441807 ,  0.16863951,  0.42527536,\n",
       "        0.7391518 , -2.2485883 , -0.6119529 , -2.0485423 ,  0.26707166,\n",
       "        0.918214  ,  0.78457505,  1.6000848 ,  0.56960005, -1.5470312 ,\n",
       "        1.7800745 , -0.20346335,  3.1684027 ,  1.4978338 ,  1.6231686 ,\n",
       "        1.0926396 , -0.3856203 , -0.23238797,  0.16016307,  0.06455152,\n",
       "       -2.6122763 ,  0.08936285, -0.26211572, -1.8819922 , -2.298521  ,\n",
       "       -0.5801686 ,  0.2587444 , -1.3541832 , -0.3041105 ,  0.09525665,\n",
       "        0.6844613 , -0.29596937, -0.77189004, -4.4422593 , -1.2630751 ,\n",
       "        1.4098878 , -0.4075041 ,  1.1187491 ,  1.4983598 , -1.1206115 ,\n",
       "        2.7006946 , -1.0008801 , -0.19859484, -0.74311805,  1.575981  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv['film']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 0.9407532215118408),\n",
       " ('documentary', 0.7449442148208618),\n",
       " ('picture', 0.7332959771156311),\n",
       " ('show', 0.7287238240242004),\n",
       " ('sequel', 0.7256034016609192),\n",
       " ('flick', 0.6875196695327759),\n",
       " ('series', 0.6664894223213196),\n",
       " ('case', 0.6515591740608215),\n",
       " ('genre', 0.6492882966995239),\n",
       " ('concept', 0.6434459686279297)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('film', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "model_wiki = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movies', 0.9322481155395508),\n",
       " ('film', 0.9310100078582764),\n",
       " ('films', 0.8937394618988037),\n",
       " ('comedy', 0.8902585506439209),\n",
       " ('hollywood', 0.8718216419219971),\n",
       " ('drama', 0.8341657519340515),\n",
       " ('sequel', 0.8222616314888),\n",
       " ('animated', 0.8216581344604492),\n",
       " ('remake', 0.812495768070221),\n",
       " ('show', 0.8105834126472473)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wiki.most_similar('movie', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic on words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.8887458443641663),\n",
       " ('woman', 0.8832994699478149),\n",
       " ('girl', 0.8797390460968018),\n",
       " ('guy', 0.8592565655708313),\n",
       " ('cop', 0.8072158694267273),\n",
       " ('boy', 0.7615447044372559),\n",
       " ('person', 0.7143130302429199),\n",
       " ('town', 0.7140844464302063),\n",
       " ('kid', 0.706073522567749),\n",
       " ('child', 0.6862486004829407)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = Word2Vec(sentences=X_train[:10000], vector_size=30, window=2, min_count=10)\n",
    "\n",
    "v_queen = word2vec.wv['queen']\n",
    "v_king = word2vec.wv['king']\n",
    "v_man = word2vec.wv['man']\n",
    "\n",
    "v_result = v_queen - v_king + v_man\n",
    "\n",
    "word2vec.wv.similar_by_vector(v_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "rnn = Sequential([\n",
    "    layers.Embedding(input_dim=5000, input_length=20, output_dim=30, mask_zero=True),\n",
    "    layers.LSTM(20),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Conv1D\n",
    "cnn = Sequential([\n",
    "    layers.Embedding(input_dim=5000, input_length=20, output_dim=30, mask_zero=True),\n",
    "    layers.Conv1D(20, kernel_size=3), # 3 words at a time\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 20, 30)            150000    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 20)                4080      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 154,101\n",
      "Trainable params: 154,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 20, 30)            150000    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 17, 20)            2420      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 340)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 341       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 152,761\n",
      "Trainable params: 152,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(rnn.summary())\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lewagon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa77d619d6dcdd36e8fd5689d2f52630df74221f02267f9c7440a3f32faeaec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
